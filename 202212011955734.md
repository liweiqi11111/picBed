# Linear regression 

housing example: 

| Living area($feet^2$) | #bedrooms | Price(1000$s) |
| --------------------- | --------- | ------------- |
| 2104                  | 3         | 400           |
| 1600                  | 3         | 330           |
| 2400                  | 3         | 369           |
| 1416                  | 2         | 232           |
| ...                   | ...       | ...           |

如何根据$x's \in \R^2$ (living area和#bedrooms) 预测$y$ (housing price)？

solution：第一步，approximate $y$ as a linear function of $x$  ($\theta_i$为参数)
$$
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2
$$
加入intercept term convention：
$$
h(x) = \sum^n_{i=0}\theta_ix_i = \theta^Tx
$$

### 如何获取或学习到参数$\theta$

第二步，定义一个cost function，用于衡量h(x) 和y的接近程度：
$$
J(\theta) = \frac{1}{2}\sum^m_{i=1}(h_\theta(x^{x(i)}) - y^{(i)})^2
$$
第三步：最小化$J(\theta)$:

#### LMS algorithm

gadient descent algorithm，重复执行下面的update rule:
$$
\theta_j := \theta_j - \alpha \frac{\part}{\part\theta_j} J(\theta)
$$
计算$\frac{\part}{\part\theta_j} J(\theta)$ ，假设只有一个training example(x, y)
$$
\begin{aligned}
\frac{\part}{\part\theta_j} J(\theta) &= \frac{\part}{\part\theta_j} \frac{1}{2}(h_\theta(x) - y)^2 \\
&= (h_\theta(x) - y) x_j
\end{aligned}
$$
得到 LMS update rule:
$$
\theta_j := \theta_j + \alpha (y^{(i)} - h_\theta(x^{(i)})) x_j^{(i)}
$$

- **batch gradient descent**

  $Repeat \ until \ convergence \{ \\
  \ \ \ \ \ \theta_j := \theta_j + \alpha (y^{(i)} - h_\theta(x^{(i)})) x_j^{(i)} \\ \}$

  

- **stochastic gradient descent**

  $Loop  \{  \\ \ \ \ for \ i=1 \ to \ m \ \{ \\ \ \ \ \ \ \ \ \ \  \theta_j := \theta_j + \alpha (y^{(i)} - h_\theta(x^{(i)})) x_j^{(i)} \ \ (for \ every \ j) \\ \ \ \  \} \\ \}$

  

#### The normal equations

$$
\begin{aligned}
\nabla_\theta J(\theta) &= \nabla_\theta \frac{1}{2} (X\theta - \overrightarrow{y})^T (X\theta - \overrightarrow{y}) \\
&= \frac{1}{2} \nabla_\theta (\theta^TX^TX\theta - \theta^TX^T\overrightarrow{y} - \overrightarrow{y}^TX\theta + \overrightarrow{y}^T\overrightarrow{y}) \\
&= \frac{1}{2} \nabla_\theta \ tr (\theta^TX^TX\theta - \theta^TX^T\overrightarrow{y} - \overrightarrow{y}^TX\theta + \overrightarrow{y}^T\overrightarrow{y}) \\
&= \frac{1}{2} \nabla_\theta (tr\ \theta^TX^TX\theta -2tr\ \overrightarrow{y}^TX\theta) \\
&=\frac{1}{2} (X^TX\theta + X^TX\theta - 2X^T\overrightarrow{y}) \\
&=X^TX\theta - X^T\overrightarrow{y}
\end{aligned}
$$

解得：
$$
\theta = (X^TX)^{-1} X^T \overrightarrow{y}
$$




#### Probabilistic interpretation

这里给出一个线性回归概率上的解释：

假设$y^{(i)} = \theta^Tx^{(i)} +\epsilon^{(i)} $，其中假设误差项$\epsilon^{(i)}$ 服从$IID$， 且 $ \epsilon^{(i)} \sim N(0, \sigma^2)  $ 

则有 $ y^{(i)} | x^{(i)};\theta \sim N(\theta^Tx^{(i)}, \sigma^2) $， 给出**likelihood** function， 表示数据$X$出现的可能性：
$$
L(\theta) = L(\theta; X, \overrightarrow{y}) = p(\overrightarrow{y}|X;\theta)
$$
要找到最好的参数$\theta$， 我们需要最大化$likelihood$， 表示在给定参数$\theta$的情况下，数据X出现的可能性最大（log likelihood $l(\theta)$）：
$$
\begin{aligned}
l(\theta) &= log \ L(\theta) \\ 
&= log \ \prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}) \\
&= m\ log \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2}·\frac{1}{2}\sum^m_{i=1} (y^{(i)} - \theta^Tx^{(i)})^2
\end{aligned}
$$
等价于最小化最开始定义的cost function$J(\theta)$



### Locally weighted linear regression

1. Fit $\theta$ to minimize $\sum_{i} w^{(i)} (y^{(i)} - \theta^Tx^{(i)})^2$
2. Output $\theta^T x$

$w^{(i)}$'s是非负的权重，通常可以设置成
$$
w^{(i)} = exp(-\frac{(x^{(i)-x})^2}{2\tau^2})
$$
直觉上，如果我们想要evaluate的$x$与$x^{(i)}$越接近，则权重越大，最小化目标就与$x^{(i)}$越相关， 如果$x$与$x^{(i)}$相隔远，则权重小，最小化目标就与$x^{(i)}$不那么相关。



#### 实践

定义cost function$J(\theta)$:
$$
J(\theta) = \frac{1}{2} \sum_{i=1}^m w^{(i)}(\theta^Tx^{(i)} - y^{(i)})^2
$$
矩阵形式可以写成
$$
J(\theta) = (X\theta - y)^T W (X\theta - y)
$$
其中$W \in R^{m\times m}$, $  W_{ij} =    \begin{cases}      \frac{1}{2}w^{(i)},  & i=j \\      0, & i \neq j   \end{cases} $

同样，我们可以求梯度，计算出最小化$\theta$的closed form：
$$
\theta = (X^TWX)^{-1}X^TWy
$$
给定数据集ds_5{train, valid, test}.csv，$x^{i}$是1维的

使用
$$
w^{(i)} = exp(-\frac{(x^{(i)-x})^2}{2\tau^2})
$$

调节不同的$\tau$ [0.03, 0.05, 0.1, 0.5]，在validation set上计算MSE value，画出预测的情况：

<img src="C:\Users\LWQ\AppData\Roaming\Typora\typora-user-images\image-20221201195001845.png" alt="image-20221201195001845" style="zoom: 50%;" />

<img src="C:\Users\LWQ\AppData\Roaming\Typora\typora-user-images\image-20221201195047460.png" alt="image-20221201195047460" style="zoom:50%;" />

<img src="C:\Users\LWQ\AppData\Roaming\Typora\typora-user-images\image-20221201195226748.png" alt="image-20221201195226748" style="zoom: 67%;" />